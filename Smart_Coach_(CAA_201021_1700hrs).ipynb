{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart Coach (CAA 201021 1700hrs).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "reAF-7dlY4L6",
        "F7dqLZ4wY6zi",
        "kgaabAaEe9rh",
        "9_y6ZIrrhZ-y",
        "4AJc_GkqjUIv",
        "5MPLOMk9ZjCe",
        "DMePgRAEl92Z",
        "cUjO-IpHmHVI",
        "-RIDHvy0nrqE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VodkaSin/Smart_Coach_Project/blob/main/Smart_Coach_(CAA_201021_1700hrs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_IbQPkPVSxR"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqABktRocVos"
      },
      "source": [
        "!pip install -q tensorflow==2.6.0 tensorflow-gpu==2.6.0\n",
        "!pip install -q gTTS\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reAF-7dlY4L6"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZZtn6HacbN7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow_docs.vis import embed # What does this do?\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from matplotlib.collections import LineCollection # What does this do?\n",
        "\n",
        "import matplotlib.patches as patches # What does this do?\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio # What does this do?\n",
        "import PIL\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "from IPython.display import display, Javascript, Image, HTML, Audio # When are all these used?\n",
        "from gtts import gTTS\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dqLZ4wY6zi"
      },
      "source": [
        "# Visualization helper functions\n",
        "\n",
        "1. Streaming APIs\n",
        "2. Recording and display videos\n",
        "3. Smart cropping\n",
        "4. Frame rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmrETs3ceMR"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyqTz_0Mccci"
      },
      "source": [
        "# JavaScript to create video stream from webcam\n",
        "def start_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span></span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      video.style.cssText = \"-moz-transform: scale(-1, 1); \\\n",
        "-webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); \\\n",
        "transform: scale(-1, 1); filter: FlipH;\";\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 192; //video.videoWidth;\n",
        "      captureCanvas.height = 192; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH5UVooPwhde"
      },
      "source": [
        "# Capture frame in bytes from streaming\n",
        "def take_frame(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBZCxBfcn9w"
      },
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\" Used in main function to convert image types\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BRG image (mirror flipped)\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply['img'].split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  img = cv2.flip(img,1)\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJcIDVNIct7C"
      },
      "source": [
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\" Used in the main function to convert numpy overlay onto the frame\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgaabAaEe9rh"
      },
      "source": [
        "## Recording and displaying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2DwJl_0gE1A"
      },
      "source": [
        "def record_video(video_path):\n",
        "  \"\"\" Called in the main function as an option to record the exercise video for later reference\n",
        "  Params:\n",
        "          video_path: a path name for storing the video\n",
        "  \"\"\"\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
        "    \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(video_path,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{video_path}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BIG70Xe9RZ"
      },
      "source": [
        "def show_video(video_path, video_width = 600):\n",
        "  \"\"\" Called when the user wants to review their workout recording and pose estimation\n",
        "  Params:\n",
        "          video_path: local\n",
        "          video_width: default = 600\n",
        "  Returns:\n",
        "          Display the video in cell\n",
        "  \"\"\"\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_y6ZIrrhZ-y"
      },
      "source": [
        "## Cropping algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnyKD2UOhqZs"
      },
      "source": [
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region, provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWcfXkPeiD8m"
      },
      "source": [
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEGB0gMSiATH"
      },
      "source": [
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsauaf9iJL3"
      },
      "source": [
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNMcvxfniLAm"
      },
      "source": [
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evfuBCWZiOGX"
      },
      "source": [
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJc_GkqjUIv"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLOcHtuAHrd"
      },
      "source": [
        "def draw_keypoints(frame, keypoints, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    points = []\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            array = cv2.circle(array, (int(kx), int(ky)), 3, (0,255,0), -1)\n",
        "\n",
        "def draw_connections(frame, keypoints, edges, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    \n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "        \n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
        "            array = cv2.line(array, (int(x1), int(y1)), (int(x2), int(y2)), (255,0,0), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww-ZBCQOxUBr"
      },
      "source": [
        "def draw_text(array, org, text, color):\n",
        "  array = cv2.putText(array, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1, cv2.LINE_AA )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MPLOMk9ZjCe"
      },
      "source": [
        "# Model algorithm\n",
        "\n",
        "1. Libraries of definitons (joints and angles), \n",
        "2. Model loading\n",
        "3. Angle calculation algorithm\n",
        "4. Squat counter logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMePgRAEl92Z"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0CrmuIQi_BY"
      },
      "source": [
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "\n",
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "\n",
        "# Note that the image is flipped, here the keys refer to the user's pov\n",
        "THREE_JOINT_INDEX = {\n",
        "    'right_shoulder (H)': (11,5,7),\n",
        "    'right_shoulder (S)': (7,5,6),\n",
        "    'left_shoulder (H)': (12,6,8),\n",
        "    'left_shoulder (S)': (8,6,5),\n",
        "    'right_elbow':(5,7,9),\n",
        "    'left_elbow':(6,8,10),\n",
        "    'right_torso':(5,11,13),\n",
        "    'left_torso':(6,12,14),\n",
        "    'right_knee':(11,13,15),\n",
        "    'left_knee':(12,14,16),\n",
        "    'right_torso turn':(6,5,11),\n",
        "    'left_torso_turn':(5,6,12)\n",
        "}\n",
        "FOUR_JOINT_INDEX ={\n",
        "    'shoulders_hips':(5,6,11,12),\n",
        "    'btw_legs':(11,13,12,14)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjO-IpHmHVI"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrDji3XgmJ6n"
      },
      "source": [
        "# Grab from TF hub and initialize interpreter\n",
        "!wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3?lite-format=tflite\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gJ9XxwmRhA"
      },
      "source": [
        "# Interpreter\n",
        "def movenet(frame):\n",
        "  \"\"\" Runs detection on an input frame\n",
        "  Params:\n",
        "          frame: 192x192 opencv BRG\n",
        "  Returns:\n",
        "          keypoints_with_scores: [index, location, confidence]\n",
        "  \"\"\"  \n",
        "  # Reshape iamge to 192x192x3\n",
        "  img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 192,192)\n",
        "  input_image = tf.cast(img, dtype=tf.float32) # Specify data type\n",
        "\n",
        "  # Setup input and output \n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # Make predictions \n",
        "  interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "  interpreter.invoke()\n",
        "  keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Fq9-q_ntg8"
      },
      "source": [
        "## Squat counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D39YEsmkZK3m"
      },
      "source": [
        "def find_side_squat(keypoints_with_scores, threshold=0.2):\n",
        "  '''1. Extract the joints needed: 5,6 (shoulders), 11,12 (hips), 13,14 (knees), 15,16 (ankles)\n",
        "     2. Compare the confidence for both sides: use only the side with higher confidence and if any \n",
        "        angle in the confidence side < threshold, output key 'Bad detection'= False\n",
        "     3. Ouput side [keypoints_with_scores(shoulder, hip, knee, ankle)]\n",
        "     Note: here left and right is the opposite of the user's left and right due to the mirroring effect.'''\n",
        "  left = [keypoints_with_scores[0][0][5],keypoints_with_scores[0][0][11],keypoints_with_scores[0][0][13],keypoints_with_scores[0][0][15]]\n",
        "  right = [keypoints_with_scores[0][0][6],keypoints_with_scores[0][0][12],keypoints_with_scores[0][0][14],keypoints_with_scores[0][0][16]]\n",
        "  left_confidence = [left[i][2] for i in range (4)]\n",
        "  right_confidence = [right[i][2] for i in range (4)]\n",
        "  if sum(left_confidence) > sum(right_confidence):\n",
        "    side = np.asarray(left)\n",
        "    confidence = left_confidence\n",
        "  else:\n",
        "    side = np.asarray(right)\n",
        "    confidence = right_confidence\n",
        "  for i in range(3):\n",
        "    if confidence[i]<threshold:\n",
        "      return None, False\n",
        "  return np.delete(side,2,1), True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOW7RdDigcpN"
      },
      "source": [
        "def squat_angles(side):\n",
        "  an1 = getAngle(side[0],side[1],side[2])\n",
        "  an2 = getAngle(side[1],side[2],side[3])\n",
        "  torso = an1 if an1 < 180 else 360-an1\n",
        "  knee = an2 if an2 < 180 else 360-an2\n",
        "  return torso, knee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auhk2YdGFK69"
      },
      "source": [
        "def squat_count(torso, knee):\n",
        "  '''Default: \n",
        "    1. knee flexion below 100 is considered ok, the user should aim for squatting\n",
        "           down deeper, below 60 is excellent mobility.\n",
        "    2. hip flexion is counted when angle smaller than 90 but should be larger than 70\n",
        "    3. getting up, the knee should be 180 in ideal case, 170 is considered OK, same for hip flexion\n",
        "    \n",
        "    Parameters:\n",
        "    angle: [hip flexion left, hip flexion right, knee flexion left, knee flexion right] list of degree\n",
        "    stage = string, indicator for the counter, if 'down' counter+1'''\n",
        "  if torso < 125:\n",
        "    stage = 'down'\n",
        "    score = 1\n",
        "    if knee < 95 and torso > 45:\n",
        "      score = 2\n",
        "  else:\n",
        "    if torso <160:\n",
        "      stage = 'down'\n",
        "    else: \n",
        "      stage = 'up'\n",
        "    score = -1\n",
        "  return stage, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RIDHvy0nrqE"
      },
      "source": [
        "## Angle calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTbIv83Yn46D"
      },
      "source": [
        "def getAngle(a, b, c):\n",
        "    ang = abs(math.degrees(math.atan2(b[1]-c[1], b[0]-c[0]) - math.atan2(b[1]-a[1], b[0]-a[0])))\n",
        "    return ang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S5loWastc9e"
      },
      "source": [
        "def getAngle2(a,b,c,d):\n",
        "  # For 4 point inputs (two crossing lines)\n",
        "  dx = c[1]-a[1]\n",
        "  dy = c[0]-a[0]\n",
        "  return getAngle(b,a,(d[0]-dx,d[1]-dy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QsayVAuP8v"
      },
      "source": [
        "def find_angle_3(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in THREE_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle(a[0],a[1],a[2])\n",
        "  return angles   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1udy1FkuRsP"
      },
      "source": [
        "def find_angle_4(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in FOUR_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle2(a[0],a[1],a[2],a[3])\n",
        "  return angles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kD1IYcwa7mV"
      },
      "source": [
        "# Smart Coach\n",
        "\n",
        "1. Streaming of mirroed web cam input with pose estimation rendered on frame\n",
        "2. Print output for immediate reading\n",
        "3. Text output for record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "eBkImQOywO3l",
        "outputId": "f17777fa-5ce0-423c-d195-74a29d39132a"
      },
      "source": [
        "start_stream()\n",
        "label_html = 'Smart Coach'  #label\n",
        "img_data = ''\n",
        "max_score = 0\n",
        "total_score = 0\n",
        "total_good = 0\n",
        "total_excellent = 0\n",
        "total_bad = 0\n",
        "combo_bad = 0\n",
        "combo_excellent = 0\n",
        "store = []\n",
        "times = np.empty(1)\n",
        "forms = []\n",
        "bad_torsos = []\n",
        "bad_knees = []\n",
        "low_torso = 0\n",
        "\n",
        "goal = input('Enter your target score: ')\n",
        "\n",
        "'''image_height, image_width = 192, 192\n",
        "crop_region = init_crop_region(image_height, image_width)'''\n",
        "while True:\n",
        "    js_reply = take_frame(label_html, img_data)\n",
        "    if not js_reply:\n",
        "      print('Streaming has been interupted')\n",
        "      break\n",
        "    start = time.time()\n",
        "    # Process Javascript Object (frame) to OpenCV BRG\n",
        "    frame = js_to_image(js_reply)\n",
        "    # Initialize render array\n",
        "    img_array= np.zeros([192,192,4], dtype=np.uint8)\n",
        "    # Run the interpreter on the BRG image\n",
        "    keypoints_with_scores = movenet(frame)\n",
        "    side, flag = find_side_squat(keypoints_with_scores)\n",
        "    if flag == False:\n",
        "      draw_text(img_array,(10,100),'Please adjust your position', (255,0,0))\n",
        "    else:\n",
        "      torso, knee = squat_angles(side)\n",
        "      if torso<45:\n",
        "        low_torso += 1\n",
        "        if low_torso == 1:\n",
        "          hunch_form = frame.copy()\n",
        "          draw_text(hunch_form,(10,35),'Hunched back: '+str(int(low_torso)), (255,255,0))\n",
        "          forms.append(hunch_form)\n",
        "        draw_text(img_array,(10,100),'Please straighten your back', (255,255,0))\n",
        "      stage, score = squat_count(torso, knee)\n",
        "      store.append([torso,knee,stage,score])\n",
        "      #draw_text(img_array,(100,120),'torso ' + str(torso), (255,0,0))\n",
        "      #draw_text(img_array,(100,140),'knee ' + str(knee), (255,0,0))\n",
        "      # Only count the highest score in one rep and avoid double counting\n",
        "      if stage =='down':\n",
        "        if score == 2 and score > max_score: \n",
        "          max_score = score\n",
        "          total_score += max_score\n",
        "          total_excellent += 1\n",
        "          combo_bad = 0\n",
        "          combo_excellent += 1\n",
        "          draw_text(img_array,(50,50),'Excellent x'+str(combo_excellent), (255,0,127))\n",
        "          if total_exellent == 1:\n",
        "            excellent_form = frame.copy()\n",
        "            draw_text(excellent_form,(10,35),'Excellent! ', (255,0,127))\n",
        "            forms.append(excellent_form)\n",
        "          if combo_excellent > 3:\n",
        "            total_score += 1\n",
        "        if score == 1 and max_score < 2:\n",
        "          max_score = score\n",
        "        if score == -1 and max_score == 0:\n",
        "          max_score = score\n",
        "          bad_form = frame.copy()\n",
        "          bad_keypoints = keypoints_with_scores\n",
        "          bad_torso = torso\n",
        "          bad_knee = knee\n",
        "      if stage == 'up':\n",
        "        if max_score == 1:\n",
        "          total_score += max_score\n",
        "          total_good += 1\n",
        "          combo_bad = 0\n",
        "          combo_excellent = 0\n",
        "          draw_text(img_array,(50,50),'Good!', (102,178,255))\n",
        "          if total_good == 1:\n",
        "            good_form = frame.copy()\n",
        "            draw_text(good_form,(10,35),'Can squat down more', (255,0,127))\n",
        "            forms.append(good_form)\n",
        "        if max_score == -1:\n",
        "          total_bad += 1\n",
        "          combo_bad +=1\n",
        "          combo_excellent = 0\n",
        "          if total_bad < 4:\n",
        "            draw_text(bad_form,(10,20),'Miss x '+str(total_bad), (255,255,0))\n",
        "            draw_keypoints(frame, bad_keypoints, bad_form, 0.2)\n",
        "            draw_connections(frame, bad_keypoints, EDGES, bad_form, 0.2)\n",
        "            draw_text(bad_form,(10,35),'Torso: '+str(int(bad_torso)), (255,255,0))\n",
        "            draw_text(bad_form,(10,50),'Knee: '+str(int(bad_knee)), (255,255,0))\n",
        "          draw_text(img_array,(50,50),'Miss x '+str(combo_bad), (0,0,255))\n",
        "          bad_torsos.append(int(bad_torso))\n",
        "          bad_knees.append(int(bad_knee))\n",
        "          bad_forms.append(bad_form)\n",
        "        score = 0\n",
        "        max_score = 0\n",
        "    if combo_bad > 3:\n",
        "      break\n",
        "    if total_score == goal:\n",
        "      draw_text(img_array,(10,100),'Goal achieved! Take a break!', (0,255,0))\n",
        "      break\n",
        "    # Rendering the keypoints and edges: add to img_array and convert to JS, uncomment if you want to see the pose estimation\n",
        "    draw_text(img_array,(10,20),'Total score: '+str(total_score), (102,255,102))\n",
        "    draw_text(img_array,(10,35),'Total count: '+str(total_good+total_excellent), (255,255,0))\n",
        "    draw_text(img_array,(10,185),'Total miss: '+str(total_bad), (0,0,255))\n",
        "    img_array[:,:,3] = (img_array.max(axis = 2)>0).astype(int)*255\n",
        "    img_data = bbox_to_bytes(img_array)\n",
        "    stop = time.time()\n",
        "    duration = stop-start\n",
        "    times = np.append(times, duration)\n",
        "    \n",
        "if total_bad>5 or combo_bad>3:\n",
        "  tts = gTTS('Game over! See feedback below!')\n",
        "else:\n",
        "  tts = gTTS('Workout completed, congratulations! See feedback below!')\n",
        "\n",
        "tts.save('1.wav')\n",
        "sound_file = '1.wav'\n",
        "Audio(sound_file, autoplay=True)\n",
        "\n",
        "print('---------WORKOUT SUMMARY---------'+'\\n')\n",
        "print('Total score     |         ',total_score)\n",
        "print('Total reps      |         ', total_good+total_excellent)\n",
        "print('Excellent reps  |         ',total_excellent)\n",
        "print('Good reps       |         ',total_good)\n",
        "print('Missed reps     |         ', total_bad,'\\n')\n",
        "\n",
        "if total_bad>2:\n",
        "  # Show screenshots of the person's bad squats x3\n",
        "  torsos = round(sum(bad_torsos)/len(bad_torsos),2)\n",
        "  knees = round(sum(bad_knees)/len(bad_knees),2)\n",
        "  print('-----------FEEDBACK-----------'+'\\n')\n",
        "  print('1. In the missed reps, your average hip flexion is',str(torsos),'degrees, aim for 70 to 95 degrees')\n",
        "  if knees > 95:\n",
        "    print('2. Your knees seem too stiff, aim for 60 to 95 degrees')\n",
        "  if low_torso != 0: \n",
        "    print('3. We also noticed that you might have been hunching your back during the excercise for',str(low_torso),'times, try to avoid that!')\n",
        "  print('\\n'+'Don\\'t worry, just try again, you will get better!')\n",
        "  \n",
        "if input('\\n'+'Check your form? (y/n): ')=='y':\n",
        "    for photo in bad_forms:\n",
        "        cv2_imshow(photo)      \n",
        "else:\n",
        "  print('Well done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span></span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      video.style.cssText = \"-moz-transform: scale(-1, 1); -webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); transform: scale(-1, 1); filter: FlipH;\";\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 192; //video.videoWidth;\n",
              "      captureCanvas.height = 192; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your target score: 2\n",
            "Streaming has been interupted\n",
            "---------WORKOUT SUMMARY---------\n",
            "\n",
            "Total score     |          0\n",
            "Total reps      |          0\n",
            "Excellent reps  |          0\n",
            "Good reps       |          0\n",
            "Missed reps     |          0 \n",
            "\n",
            "\n",
            "Check your form? (y/n): n\n",
            "Well done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Nw4lNDbS0U"
      },
      "source": [
        "# Data collection\n",
        "\n",
        "1. Testing of frame rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "cLuNA1Z141TE",
        "outputId": "1ff23214-8918-4bb0-eeda-47bbc126f58b"
      },
      "source": [
        "points = np.size(times)\n",
        "print('Collected points:', points)\n",
        "sorted = np.sort(times)\n",
        "print('Average loop time(s):', np.mean(sorted))\n",
        "print('Standard deviation(s):', np.std(sorted))\n",
        "plt.hist(sorted, density=True, bins=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected points: 185\n",
            "Average loop time(s): 0.03977726472390664\n",
            "Standard deviation(s): 0.0031075310674482546\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ+klEQVR4nO3dfYxldX3H8fen4EOKWsAdCeHBAbOagmkXnSCJYmjpA2Ar0DYUYhAt7UoKiUZNRUwrNTEhViQ1bTFroEAiCIpUErGVEJWYFnQWERYBXXAJu13ZERvEYm2Bb/+YM3KZvevcmXvvPPz2/UpO5pzfebjf+e3dz5793XPPSVUhSWrLr6x0AZKk0TPcJalBhrskNchwl6QGGe6S1KB9V7oAgHXr1tXk5ORKlyFJa8rmzZt/VFUT/datinCfnJxkenp6pcuQpDUlySN7WuewjCQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWhVfEO1dZMXfmmg7bZd8pYxVyJpb+GZuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBC4Z7ksOSfDXJd5Pcl+TdXfuBSW5N8v3u5wFde5J8MsnWJPcked24fwlJ0vMNcub+NPC+qjoKOA44P8lRwIXAbVW1HritWwY4GVjfTRuBy0detSTpl1ow3KtqZ1Xd1c0/CdwPHAKcClzdbXY1cFo3fypwTc26A9g/ycEjr1yStEeLGnNPMgkcA9wJHFRVO7tVPwQO6uYPAR7t2W171yZJWiYDh3uSlwA3Au+pqp/0rquqAmoxL5xkY5LpJNMzMzOL2VWStICBwj3JC5gN9s9U1Re65sfmhlu6n7u69h3AYT27H9q1PU9VbaqqqaqampiYWGr9kqQ+BrlaJsAVwP1V9YmeVTcD53Tz5wBf7Gl/e3fVzHHAEz3DN5KkZTDI/dzfCJwN3Jvk7q7tIuAS4IYk5wKPAGd0624BTgG2Ak8B7xxpxZKkBS0Y7lX1DSB7WH1in+0LOH/IuiRJQ/AbqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBg3ymL0rk+xKsqWn7fokd3fTtrknNCWZTPKznnWfGmfxkqT+BnnM3lXAPwDXzDVU1Z/OzSe5FHiiZ/uHqmrDqAocpckLvzTQdtsuecuYK+lvtdcnae0Y5DF7tyeZ7Leue3j2GcBvj7YsSdIwhh1zPx54rKq+39N2RJJvJ/l6kuP3tGOSjUmmk0zPzMwMWYYkqdew4X4WcF3P8k7g8Ko6BngvcG2Sl/Xbsao2VdVUVU1NTEwMWYYkqdeSwz3JvsAfAdfPtVXVz6vq8W5+M/AQ8Ophi5QkLc4wZ+6/AzxQVdvnGpJMJNmnmz8SWA88PFyJkqTFGuRSyOuA/wBek2R7knO7VWfy/CEZgDcD93SXRn4eOK+qfjzKgiVJCxvkapmz9tD+jj5tNwI3Dl+WJGkYfkNVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDXLLX60yg94aGLw9sLS38sxdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchLIftYzKWGkrQaDfKwjiuT7Eqypaft4iQ7ktzdTaf0rPtgkq1JHkzy++MqXJK0Z4MMy1wFnNSn/bKq2tBNtwAkOYrZJzQd3e3zT3OP3ZMkLZ8Fw72qbgcGfVTeqcBnuwdl/wDYChw7RH2SpCUY5gPVC5Lc0w3bHNC1HQI82rPN9q5tN0k2JplOMj0zMzNEGZKk+ZYa7pcDrwI2ADuBSxd7gKraVFVTVTU1MTGxxDIkSf0sKdyr6rGqeqaqngU+zXNDLzuAw3o2PbRrkyQtoyWFe5KDexZPB+aupLkZODPJi5IcAawHvjlciZKkxVrwOvck1wEnAOuSbAc+DJyQZANQwDbgXQBVdV+SG4DvAk8D51fVM+MpXZK0JwuGe1Wd1af5il+y/UeBjw5TlCRpON5+QJIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYtGO7dA7B3JdnS0/Z3SR7oHpB9U5L9u/bJJD9Lcnc3fWqcxUuS+hvkzP0q4KR5bbcCr62q3wC+B3ywZ91DVbWhm84bTZmSpMVYMNyr6nbgx/PavlJVT3eLdzD7IGxJ0ioxijH3PwO+3LN8RJJvJ/l6kuP3tFOSjUmmk0zPzMyMoAxJ0pyhwj3Jh5h9EPZnuqadwOFVdQzwXuDaJC/rt29VbaqqqaqampiYGKYMSdI8Sw73JO8A/gB4W1UVQFX9vKoe7+Y3Aw8Brx5BnZKkRVhSuCc5Cfgr4K1V9VRP+0SSfbr5I4H1wMOjKFSSNLh9F9ogyXXACcC6JNuBDzN7dcyLgFuTANzRXRnzZuAjSf4PeBY4r6p+3PfAkqSxWTDcq+qsPs1X7GHbG4Ebhy1KkjQcv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQQOGe5Moku5Js6Wk7MMmtSb7f/Tyga0+STybZmuSeJK8bV/GSpP4GPXO/CjhpXtuFwG1VtR64rVsGOJnZZ6euBzYClw9fpiRpMQYK96q6HZj/LNRTgau7+auB03rar6lZdwD7Jzl4FMVKkgYzzJj7QVW1s5v/IXBQN38I8GjPdtu7tudJsjHJdJLpmZmZIcqQJM03kg9Uq6qAWuQ+m6pqqqqmJiYmRlGGJKkzTLg/Njfc0v3c1bXvAA7r2e7Qrk2StEyGCfebgXO6+XOAL/a0v727auY44Ime4RtJ0jLYd5CNklwHnACsS7Id+DBwCXBDknOBR4Azus1vAU4BtgJPAe8ccc2SpAUMFO5VddYeVp3YZ9sCzh+mKEnScPyGqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0a6H7u/SR5DXB9T9ORwN8A+wN/Acw99fqiqrplyRVKkhZtyeFeVQ8CGwCS7MPsc1JvYvbJS5dV1cdHUqEkadFGNSxzIvBQVT0youNJkoYwqnA/E7iuZ/mCJPckuTLJAf12SLIxyXSS6ZmZmX6bSJKWaOhwT/JC4K3A57qmy4FXMTtksxO4tN9+VbWpqqaqampiYmLYMiRJPUZx5n4ycFdVPQZQVY9V1TNV9SzwaeDYEbyGJGkRRhHuZ9EzJJPk4J51pwNbRvAakqRFWPLVMgBJ9gN+F3hXT/PHkmwACtg2b50kaRkMFe5V9d/Ay+e1nT1URZKkofkNVUlqkOEuSQ0y3CWpQUONua8Wkxd+aaVLkKRVxTN3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQUPfWybJNuBJ4Bng6aqaSnIgcD0wyewDO86oqv8a9rUkSYMZ1Zn7b1XVhqqa6pYvBG6rqvXAbd2yJGmZjOuukKcCJ3TzVwNfAz4wptfSLzHoHTO3XfKWMVciaTmN4sy9gK8k2ZxkY9d2UFXt7OZ/CBw0f6ckG5NMJ5memZkZQRmSpDmjOHN/U1XtSPIK4NYkD/SurKpKUvN3qqpNwCaAqamp3dZLkpZu6DP3qtrR/dwF3AQcCzyW5GCA7ueuYV9HkjS4ocI9yX5JXjo3D/wesAW4GTin2+wc4IvDvI4kaXGGHZY5CLgpydyxrq2qf03yLeCGJOcCjwBnDPk6kqRFGCrcq+ph4Df7tD8OnDjMsSVJS+c3VCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatC47gqpNWal7h7pXSul8fDMXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBi053JMcluSrSb6b5L4k7+7aL06yI8nd3XTK6MqVJA1imC8xPQ28r6ru6h61tznJrd26y6rq48OXJ0laiiWHe1XtBHZ2808muR84ZFSFSZKWbiRj7kkmgWOAO7umC5Lck+TKJAeM4jUkSYMbOtyTvAS4EXhPVf0EuBx4FbCB2TP7S/ew38Yk00mmZ2Zmhi1DktRjqHBP8gJmg/0zVfUFgKp6rKqeqapngU8Dx/bbt6o2VdVUVU1NTEwMU4YkaZ4lj7knCXAFcH9VfaKn/eBuPB7gdGDLcCVqNWnp7pHekVItG+ZqmTcCZwP3Jrm7a7sIOCvJBqCAbcC7hqpQkrRow1wt8w0gfVbdsvRyJEmj4DdUJalBhrskNchwl6QGGe6S1CDDXZIaNMylkNIeDXoN+Vrg9fBaizxzl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3yUkg1ZSUvwRz1JZNegrl8FvO+WSt/fp65S1KDDHdJapDhLkkNGlu4JzkpyYNJtia5cFyvI0na3VjCPck+wD8CJwNHMfvovaPG8VqSpN2N68z9WGBrVT1cVf8LfBY4dUyvJUmaJ1U1+oMmfwKcVFV/3i2fDbyhqi7o2WYjsLFbfA3wYM8h1gE/Gnlha5t9sjv7ZHf2ye5a7pNXVtVEvxUrdp17VW0CNvVbl2S6qqaWuaRVzT7ZnX2yO/tkd3trn4xrWGYHcFjP8qFdmyRpGYwr3L8FrE9yRJIXAmcCN4/ptSRJ84xlWKaqnk5yAfBvwD7AlVV13yIO0Xe4Zi9nn+zOPtmdfbK7vbJPxvKBqiRpZfkNVUlqkOEuSQ1alnBf6FYESV6U5Ppu/Z1JJrv2Y5Pc3U3fSXJ6zz7bktzbrZtejt9jlJbaJz3rD0/y0yTvH/SYq92Y+mSvfJ8kmUzys56/P5/q2ef1XZ9sTfLJJFm+32h4Y+qTr3XHnFv3iuX7jcakqsY6MfuB6kPAkcALge8AR83b5i+BT3XzZwLXd/O/CuzbzR8M7OpZ3gasG3f9q61PetZ/Hvgc8P5Bj7map3H0yd78PgEmgS17OO43geOAAF8GTl7p33UV9MnXgKmV/v1GOS3HmfsgtyI4Fbi6m/88cGKSVNVTVfV01/5ioJVPf5fcJwBJTgN+APRegbTWb/kwjj5Z64bqk36SHAy8rKruqNlUuwY4bfSlj83I+6RVyxHuhwCP9ixv79r6btOF+RPAywGSvCHJfcC9wHk9YV/AV5Js7m5lsJYsuU+SvAT4APC3SzjmajaOPoG99H3SrTsiybeTfD3J8T3bb1/gmKvZOPpkzj93QzJ/3cI/Bqv+MXtVdSdwdJJfB65O8uWq+h/gTVW1oxsbuzXJA1V1+8pWuywuBi6rqp828P4blYvZc5/sre+TncDhVfV4ktcD/5Lk6JUuaoX17ZOq+gnwtu598lLgRuBsZv9Xs2Ytx5n7ILci+MU2SfYFfg14vHeDqrof+Cnw2m55R/dzF3ATs/9dWyuG6ZM3AB9Lsg14D3BRZr8wttZv+TCOPtlr3ydV9fOqehygqjYzO0796m77Qxc45mo2jj7pfZ88CVzL2nqf9DfuQX1m/3fwMHAEz30AcvS8bc7n+R+A3NDNH8FzH6C+EvhPZu/wth/w0q59P+Dfmb0L5Yp/iDHuPpm3zcU894HqgsdczdOY+mSvfZ8AE8A+3fyRzAbegd3y/A9UT1np33Ul+6Q75rqu/QXMjtOft9K/69B9tUx/IKcA32P2X8oPdW0fAd7azb+Y2asctnZvvCO79rOZ/YDsbuAu4LSeP5jvdNN9c8dcS9NS+2TeMX4RZHs65lqaRt0ne/P7BPjjeX93/rDnmFPAlu6Y/0D3TfW1Mo26T5j9h38zcE+3/u/p/hFYy5O3H5CkBvkNVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGvT/+wdTgiLZPz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxDhmweu8Cjs",
        "outputId": "1263bbac-69b7-406c-8890-3efe9833318e"
      },
      "source": [
        "print(store)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ]
    }
  ]
}